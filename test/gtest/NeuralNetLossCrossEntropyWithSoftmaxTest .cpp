#include <stdio.h>
#include <iostream>
#include "gtest/gtest.h"
#include "bb/NeuralNetLossCrossEntropyWithSoftmax.h"



/*
pyton で作った期待値
x = [[0.03682449 0.7174495  0.9130372  0.6275032  0.11995261 0.25850186
0.9942376  0.23910977 0.75574166 0.38816985]
[0.59729785 0.7654758  0.35820892 0.0122047  0.05050702 0.323845
0.6889169  0.16161442 0.06116614 0.769033]
[0.94036084 0.25700864 0.73131865 0.58721095 0.78960747 0.29338115
0.36369365 0.5750651  0.81494033 0.05262487]
[0.98256797 0.15146895 0.51034707 0.38718516 0.18072742 0.7927627
0.13334593 0.03729849 0.20618507 0.9241972]
[0.713539   0.48547512 0.27168426 0.54288715 0.6140953  0.67773014
0.03847398 0.7239832  0.64887875 0.42749837]]
t = [2 5 6 3 4]
less = 2.297830009460449
dx = [[0.01189111  0.02348626 - 0.17144011  0.02146598  0.01292184  0.0148421
0.03097572  0.01455706  0.02440304  0.016897]
[0.02385998  0.02822986  0.01878603  0.01329129  0.01381026 - 0.18184856
0.02614928  0.01543317  0.01395825  0.02833046]
[0.02876907  0.01452612  0.0233421   0.02020946  0.02474312  0.0150642
- 0.18383847  0.01996549  0.02537794  0.01184096]
[0.03275547  0.01426731  0.02042683 - 0.18194023  0.01469092  0.02709271
0.01401107  0.01272795  0.01506971  0.03089824]
[0.02392219  0.01904382  0.01537821  0.02016916 - 0.17834225  0.02308072
0.01217935  0.02417335  0.02242433  0.01797112]]
*/


TEST(NeuralNetLossCrossEntropyWithSoftmaxTest, testNeuralNetLossCrossEntropyWithSoftmax)
{
	const float x_table[5][10] = {
		{ 0.03682449f, 0.7174495f, 0.9130372f, 0.6275032f, 0.11995261f, 0.25850186f, 0.9942376f, 0.23910977f, 0.75574166f, 0.38816985f },
		{ 0.59729785f, 0.7654758f, 0.35820892f, 0.0122047f, 0.05050702f, 0.323845f, 0.6889169f, 0.16161442f, 0.06116614f, 0.769033f },
		{ 0.94036084f, 0.25700864f, 0.73131865f, 0.58721095f, 0.78960747f, 0.29338115f, 0.36369365f, 0.5750651f, 0.81494033f, 0.05262487f },
		{0.98256797f, 0.15146895f, 0.51034707f, 0.38718516f, 0.18072742f, 0.7927627f, 0.13334593f, 0.03729849f, 0.20618507f, 0.9241972f},
		{0.713539f, 0.48547512f, 0.27168426f, 0.54288715f, 0.6140953f, 0.67773014f, 0.03847398f, 0.7239832f, 0.64887875f, 0.42749837f}, };
	const int x_label[5] = { 2, 5, 6, 3, 4 };
	const float dx_table[5][16] = {
		{ 0.01189111f, 0.02348626f, -0.17144011f, 0.02146598f, 0.01292184f, 0.0148421f, 0.03097572f, 0.01455706f, 0.02440304f, 0.016897f },
		{0.02385998f, 0.02822986f, 0.01878603f, 0.01329129f, 0.01381026f, -0.18184856f, 0.02614928f, 0.01543317f, 0.01395825f, 0.02833046f},
		{0.02876907f, 0.01452612f, 0.0233421f, 0.02020946f, 0.02474312f, 0.0150642f, -0.18383847f, 0.01996549f, 0.02537794f, 0.01184096f},
		{0.03275547f, 0.01426731f, 0.02042683f, -0.18194023f, 0.01469092f, 0.02709271f, 0.01401107f, 0.01272795f, 0.01506971f, 0.03089824f},
		{0.02392219f, 0.01904382f, 0.01537821f, 0.02016916f, -0.17834225f, 0.02308072f, 0.01217935f, 0.02417335f, 0.02242433f, 0.01797112f}, };

	
	std::vector< std::vector<float> >	x(5, std::vector<float>(16));
	std::vector< std::vector<float> >	t(5, std::vector<float>(16, 0.0f));
	std::vector< std::vector<float> >	dx(5, std::vector<float>(16));

	for (int i = 0; i < 5; i++) {
		for (int j = 0; j < 10; j++) {
			x[i][j] = x_table[i][j];
			t[i][j] = (j == x_label[i]) ? 1.0f : 0.0f;
			dx[i][j] = dx_table[i][j];
		}
	}
	

	bb::NeuralNetBuffer<> buf_sig(5, 10, BB_TYPE_REAL32);
	bb::NeuralNetBuffer<> buf_err(5, 10, BB_TYPE_REAL32);
	for (int i = 0; i < 5; i++) {
		for (int j = 0; j < 10; j++) {
			buf_sig.SetReal(i, j, x[i][j]);
		}
	}

	bb::NeuralNetLossCrossEntropyWithSoftmax<> lossFunc;

	double loss = lossFunc.CalculateLoss(buf_sig, buf_err, t.begin());

//	std::cout << "loss = " << loss << " (exp:2.297830009460449)" << std::endl;
	EXPECT_TRUE(abs(loss - 2.297830009460449) < 0.00001);

	for (int i = 0; i < 5; i++) {
		for (int j = 0; j < 10; j++) {
//			std::cout << "err : " << buf_err.GetReal(i, j)  << " exp : " << dx[i][j] << std::endl;
			EXPECT_TRUE(abs(buf_err.GetReal(i, j) - dx[i][j]) < 0.00001);
		}
	}
}


