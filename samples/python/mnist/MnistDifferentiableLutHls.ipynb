{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微分可能LUTモデルによるMNIST学習のHLSサンプル\n",
    "\n",
    "Stochasticモデルに BatchNormalization や Binarize(backward時はHard-Tanh)を加えることで、より一般的なデータに対してLUT回路学習を行います。\n",
    "ここでは HLS に出力することを目的にシンプルな多層パーセプトロンモデルを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import binarybrain as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryBrain : 4.2.5\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "print('BinaryBrain : %s'%bb.get_version_string())\n",
    "#bb.set_host_only(True)\n",
    "print(bb.get_device_name(0))\n",
    "bb.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "異なる閾値で2値化した画像でフレーム数を水増ししながら学習させます。この水増しをバイナリ変調と呼んでいます。\n",
    "\n",
    "ここではフレーム方向の水増し量を frame_modulation_size で指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "data_path             = './data/'\n",
    "net_name              = 'MnistDifferentiableLutHls'\n",
    "data_path             = os.path.join('./data/', net_name)\n",
    "hls_function_name     = 'MnistLut'\n",
    "hls_output_file       = os.path.join(data_path, net_name + '.h')\n",
    "hls_src_path          = '../../hls/mnist/simple/src'\n",
    "hls_src_file          = os.path.join(hls_src_path, net_name + '.h')\n",
    "hls_testbench_path    = '../../hls/mnist/simple/testbench'\n",
    "hls_testdata_file     = os.path.join(hls_testbench_path, 'mnist_test_data.h')\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "\n",
    "epochs                = 16\n",
    "mini_batch_size       = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットは PyTorch の torchvision を使います。ミニバッチのサイズも DataLoader で指定しています。\n",
    "BinaryBrainではミニバッチをフレーム数として FrameBufferオブジェクトで扱います。\n",
    "バイナリ変調で計算中にフレーム数が変わるためデータセットの準備観点でのミニバッチと呼び分けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset_path = './data/'\n",
    "dataset_train = torchvision.datasets.MNIST(root=dataset_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "dataset_test  = torchvision.datasets.MNIST(root=dataset_path, train=False, transform=transforms.ToTensor(), download=True)\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=mini_batch_size, shuffle=True, num_workers=2)\n",
    "loader_test  = torch.utils.data.DataLoader(dataset=dataset_test,  batch_size=mini_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの構築\n",
    "\n",
    "DifferentiableLut に特に何もオプションをつけなければOKです。<br>\n",
    "バイナリ変調を施すためにネットの前後に RealToBinary層とBinaryToReal層を入れています。<br>\n",
    "send_command で \"binary true\" を送ることで、DifferentiableLut の内部の重み係数が 0.0-1.0 の間に拘束されます。\n",
    "\n",
    "接続数がLUTの物理構成に合わせて、1ノード当たり6個なので層間で6倍以上ノード数が違うと接続されないノードが発生するので、注意してネットワーク設計が必要です。\n",
    "最終段は各クラス7個の結果を出して Reduce で足し合わせています。こうすることで若干の改善がみられるとともに、加算結果が INT3 相当になるために若干尤度を数値的に見ることができるようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "[Sequential] \n",
      " input  shape : [1, 28, 28] output shape : [10]\n",
      "  --------------------------------------------------------------------\n",
      "  [Binarize] \n",
      "   input  shape : {1, 28, 28} output shape : {1, 28, 28}\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {1, 28, 28} output shape : {256}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {256} output shape : {128}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {128} output shape : {10, 64}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DepthwiseDenseAffineQuantize] \n",
      "   input  shape : {10, 64} output shape : {10}\n",
      "   input(64, 10) output(1, 10)\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "net = bb.Sequential([\n",
    "            bb.Binarize(binary_th=0.5, binary_low=0.0, binary_high=1.0),\n",
    "            bb.DifferentiableLut([256]),\n",
    "            bb.DifferentiableLut([128]),\n",
    "            bb.DifferentiableLut([10, 64]),\n",
    "            bb.DepthwiseDenseAffineQuantize([10]),\n",
    "        ])\n",
    "\n",
    "net.set_input_shape([1, 28, 28])\n",
    "\n",
    "net.send_command(\"binary true\")\n",
    "\n",
    "loss      = bb.LossSoftmaxCrossEntropy()\n",
    "metrics   = bb.MetricsCategoricalAccuracy()\n",
    "optimizer = bb.OptimizerAdam(learning_rate=0.0001)\n",
    "\n",
    "net.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実施\n",
    "\n",
    "load_networks/save_networks で途中結果を保存/復帰可能できます。ネットワークの構造が変わると正常に読み込めなくなるので注意ください。\n",
    "(その場合は新しいネットをsave_networksするまで一度load_networks をコメントアウトください)\n",
    "\n",
    "tqdm などを使うと学習過程のプログレス表示ができて便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 285.85it/s, acc=0.783, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0] : loss=0.827466 accuracy=0.857700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 270.40it/s, acc=0.882, loss=0.612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1] : loss=0.528281 accuracy=0.886000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 269.86it/s, acc=0.897, loss=0.445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[2] : loss=0.455555 accuracy=0.894200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 274.94it/s, acc=0.907, loss=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[3] : loss=0.467462 accuracy=0.874600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 263.25it/s, acc=0.912, loss=0.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[4] : loss=0.361713 accuracy=0.909700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 271.88it/s, acc=0.918, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[5] : loss=0.322284 accuracy=0.908200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 268.21it/s, acc=0.922, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[6] : loss=0.341989 accuracy=0.904700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 269.88it/s, acc=0.923, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[7] : loss=0.317013 accuracy=0.910600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 256.40it/s, acc=0.924, loss=0.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[8] : loss=0.293453 accuracy=0.915600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 263.18it/s, acc=0.928, loss=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[9] : loss=0.333258 accuracy=0.900600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 267.21it/s, acc=0.93, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[10] : loss=0.277272 accuracy=0.914800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 263.37it/s, acc=0.93, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[11] : loss=0.269285 accuracy=0.920600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 261.30it/s, acc=0.932, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[12] : loss=0.291270 accuracy=0.916700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 261.68it/s, acc=0.932, loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[13] : loss=0.296566 accuracy=0.907300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 260.69it/s, acc=0.934, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[14] : loss=0.246316 accuracy=0.927800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 250.04it/s, acc=0.934, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[15] : loss=0.236590 accuracy=0.927500\n"
     ]
    }
   ],
   "source": [
    "# bb.load_networks(data_path, net)\n",
    "\n",
    "# learning\n",
    "optimizer.set_variables(net.get_parameters(), net.get_gradients())\n",
    "for epoch in range(epochs):\n",
    "    # learning\n",
    "    loss.clear()\n",
    "    metrics.clear()\n",
    "    with tqdm(loader_train) as t:\n",
    "        for images, labels in t:\n",
    "            x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "            t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "\n",
    "            y_buf = net.forward(x_buf, train=True)\n",
    "\n",
    "            dy_buf = loss.calculate(y_buf, t_buf)\n",
    "            metrics.calculate(y_buf, t_buf)\n",
    "\n",
    "            net.backward(dy_buf)\n",
    "\n",
    "            optimizer.update()\n",
    "        \n",
    "            t.set_postfix(loss=loss.get(), acc=metrics.get())\n",
    "\n",
    "    # test\n",
    "    loss.clear()\n",
    "    metrics.clear()\n",
    "    for images, labels in loader_test:\n",
    "        x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "        t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "\n",
    "        y_buf = net.forward(x_buf, train=False)\n",
    "\n",
    "        loss.calculate(y_buf, t_buf)\n",
    "        metrics.calculate(y_buf, t_buf)\n",
    "    print('epoch[%d] : loss=%f accuracy=%f' % (epoch, loss.get(), metrics.get()))\n",
    "    \n",
    "    bb.save_networks(data_path, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGA用HLS(C言語高位合成)で使う為の出力\n",
    "\n",
    "内部データを取得する例としてHSL(C言語高位合成)用の出力を作ってみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みを読みなおす\n",
    "bb.load_networks(data_path, net)\n",
    "\n",
    "# HLSソースを出力\n",
    "with open(hls_output_file, \"w\") as f:\n",
    "    # header\n",
    "    f.write('// BinaryBrain MnistDifferentiableLut HLS sample\\n\\n')\n",
    "    f.write('#include \"ap_int.h\"\\n\\n')\n",
    "    \n",
    "    # LUT-Net 出力\n",
    "    for i in range(1, 4):\n",
    "        bb.dump_hls_lut_layer(f, hls_function_name + \"_layer%d\"%i, net[i])\n",
    "    f.write('\\n\\n')\n",
    "    \n",
    "    # DenseAffine parameter\n",
    "    W = (net[4].WQ().numpy() * 256).astype(np.int32)\n",
    "    b = (net[4].bQ().numpy() * 256).astype(np.int32)\n",
    "    f.write('const int DWA_DEPTH = %d;\\n'%W.shape[2])\n",
    "    f.write('const ap_int<8> W_tbl[%d][DWA_DEPTH] =\\n'%(W.shape[0]))\n",
    "    f.write('    {\\n')\n",
    "    for i in range(W.shape[0]):\n",
    "        f.write('        {')\n",
    "        for j in range(W.shape[2]):\n",
    "            f.write('%5d, '%W[i][0][j])\n",
    "        f.write('},\\n')\n",
    "    f.write('    };\\n\\n')\n",
    "    \n",
    "    f.write('const ap_int<8> b_tbl[DWA_DEPTH] = {')\n",
    "    for i in range(b.shape[0]):\n",
    "        f.write('%5d, '%b[i])\n",
    "    f.write('};\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../hls/mnist/simple/src/MnistDifferentiableLutHls.h'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulation用ファイルに上書きコピー\n",
    "shutil.copyfile(hls_output_file, hls_src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストベンチ用データ作成\n",
    "tests = 20\n",
    "\n",
    "for images, labels in loader_test:\n",
    "    break\n",
    "with open(hls_testdata_file, \"w\") as f:\n",
    "    f.write('\\n')\n",
    "    f.write('unsigned int test_size = %d;\\n'%tests)\n",
    "    f.write('unsigned int test_images[%d][28][28] = {\\n'%tests)\n",
    "    for i in range(tests):\n",
    "        f.write('    {\\n')\n",
    "        for y in range(28):\n",
    "            f.write('        {')\n",
    "            for x in range(28):\n",
    "                if images[i][0][y][x] > 0.5:\n",
    "                    f.write('1,')\n",
    "                else:\n",
    "                    f.write('0,')\n",
    "            f.write('},\\n')\n",
    "        f.write('    },\\n')\n",
    "    f.write('};\\n\\n')\n",
    "    \n",
    "    f.write('unsigned int test_labels[%d] = {'%tests)\n",
    "    for i in range(tests):\n",
    "        f.write('%d,'%labels[i])\n",
    "    f.write('};\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bb_cur",
   "language": "python",
   "name": "bb_cur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42ac7cce52f856a953029bf8d6268c151b6ce75ee0dd17c7a9f0cf75b5e0010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
