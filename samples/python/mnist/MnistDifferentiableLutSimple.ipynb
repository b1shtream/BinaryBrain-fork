{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微分可能LUTモデルによるMNIST学習\n",
    "\n",
    "Stochasticモデルに BatchNormalization や Binarize(backward時はHard-Tanh)を加えることで、より一般的なデータに対してLUT回路学習を行います。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import binarybrain as bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "異なる閾値で2値化した画像でフレーム数を水増ししながら学習させます。この水増しをバイナリ変調と呼んでいます。\n",
    "\n",
    "ここではフレーム方向の水増し量を frame_modulation_size で指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "data_path             = './data/'\n",
    "net_name              = 'MnistDifferentiableLutSimple'\n",
    "save_path             = os.path.join(data_path, net_name)\n",
    "rtl_sim_path          = '../../verilog/mnist'\n",
    "rtl_module_name       = 'MnistLutSimple'\n",
    "output_velilog_file   = os.path.join(data_path, net_name + '.v')\n",
    "sim_velilog_file      = os.path.join(rtl_sim_path, rtl_module_name + '.v')\n",
    "\n",
    "epochs                = 0\n",
    "frame_modulation_size = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットは PyTorch の torchvision を使います。ミニバッチのサイズは64です。\n",
    "BinaryBrainではミニバッチをフレーム数として FrameBufferオブジェクトで扱います。\n",
    "バイナリ変調で計算中にフレーム数が変わるためデータセットの準備観点でのミニバッチと呼び分けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset_train = torchvision.datasets.MNIST(root=data_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "dataset_test  = torchvision.datasets.MNIST(root=data_path, train=False, transform=transforms.ToTensor(), download=True)\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "loader_test  = torch.utils.data.DataLoader(dataset=dataset_test,  batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの構築\n",
    "\n",
    "DifferentiableLut に特に何もオプションをつけなければOKです。<br>\n",
    "バイナリ変調を施すためにネットの前後に RealToBinary層とBinaryToReal層を入れています。<br>\n",
    "send_command で \"binary true\" を送ることで、DifferentiableLut の内部の重み係数が 0.0-1.0 の間に拘束されます。\n",
    "\n",
    "接続数がLUTの物理構成に合わせて、1ノード当たり6個なので層間で6倍以上ノード数が違うと接続されないノードが発生するので、注意してネットワーク設計が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "net = bb.Sequential([\n",
    "            bb.RealToBinary(frame_modulation_size=frame_modulation_size),\n",
    "            bb.DifferentiableLut([1024]),\n",
    "            bb.DifferentiableLut([420]),\n",
    "            bb.DifferentiableLut([70]),\n",
    "            bb.Reduce([10]),\n",
    "            bb.BinaryToReal(frame_modulation_size=frame_modulation_size)\n",
    "        ])\n",
    "net.set_input_shape([1, 28, 28])\n",
    "\n",
    "net.send_command(\"binary true\")\n",
    "\n",
    "loss      = bb.LossSoftmaxCrossEntropy()\n",
    "metrics   = bb.MetricsCategoricalAccuracy()\n",
    "optimizer = bb.OptimizerAdam()\n",
    "\n",
    "optimizer.set_variables(net.get_parameters(), net.get_gradients())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実施\n",
    "\n",
    "load_networks/save_networks で途中結果を保存/復帰可能できます。ネットワークの構造が変わると正常に読み込めなくなるので注意ください。\n",
    "(その場合は新しいネットをsave_networksするまで一度load_networks をコメントアウトください)\n",
    "\n",
    "tqdm などを使うと学習過程のプログレス表示ができて便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb.load_networks(save_path, net)\n",
    "\n",
    "# learning\n",
    "for epoch in range(epochs):\n",
    "    # learning\n",
    "    loss.clear()\n",
    "    metrics.clear()\n",
    "    with tqdm(loader_train) as t:\n",
    "        for images, labels in t:\n",
    "            x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "            t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "\n",
    "            y_buf = net.forward(x_buf, train=True)\n",
    "            \n",
    "            dy_buf = loss.calculate(y_buf, t_buf)\n",
    "            metrics.calculate(y_buf, t_buf)\n",
    "            \n",
    "            net.backward(dy_buf)\n",
    "\n",
    "            optimizer.update()\n",
    "            \n",
    "            t.set_postfix(loss=loss.get(), acc=metrics.get())\n",
    "\n",
    "    # test\n",
    "    loss.clear()\n",
    "    metrics.clear()\n",
    "    for images, labels in loader_test:\n",
    "        x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "        t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "\n",
    "        y_buf = net.forward(x_buf, train=False)\n",
    "\n",
    "        loss.calculate(y_buf, t_buf)\n",
    "        metrics.calculate(y_buf, t_buf)\n",
    "\n",
    "    print('epoch[%d] : loss=%f accuracy=%f' % (epoch, loss.get(), metrics.get()))\n",
    "\n",
    "    bb.save_networks(save_path, net, keep_olds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGA用Verilog出力\n",
    "\n",
    "最後に学習したネットワークを Verilog 出力します。\n",
    "MNISTのサイズである 28x28=784bit の入力を 10bit の分類をして出力するだけのシンプルなモジュールを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ./data/MnistDifferentiableLutSimple\\20201230_161118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../verilog/mnist\\\\MnistLutSimple.v'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export verilog\n",
    "bb.load_networks(save_path, net)\n",
    "\n",
    "# 結果を出力\n",
    "with open(output_velilog_file, 'w') as f:\n",
    "    f.write('`timescale 1ns / 1ps\\n\\n')\n",
    "    bb.dump_verilog_lut_layers(f, module_name=rtl_module_name, net=net)\n",
    "\n",
    "# Simulation用ファイルに上書きコピー\n",
    "shutil.copyfile(output_velilog_file, sim_velilog_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション用データファイル作成\n",
    "with open(os.path.join(rtl_sim_path, 'mnist_test.txt'), 'w') as f:\n",
    "    bb.dump_verilog_readmemb_image_classification(f ,loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの内部の値を取得する\n",
    "\n",
    "Verilog以外の言語やFPGA以外に適用したい場合、接続とLUTテーブルの2つが取得できれば同じ計算をするモデルをインプリメントすることが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前準備\n",
    "そのままだと勾配はリセットされているので少しだけ逆伝搬を実施します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load : ./data/MnistDifferentiableLutSimple\\20201230_161118\n"
     ]
    }
   ],
   "source": [
    "# 最新の保存データ読み込み\n",
    "bb.load_networks(save_path, net)\n",
    "\n",
    "# layer を取り出す\n",
    "layer0 = net[1]\n",
    "layer1 = net[2]\n",
    "layer2 = net[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接続を取得する\n",
    "\n",
    "LUTモデルは get_connection_list() にて接続行列を取得できます。<br>\n",
    "ここでの各出力ノードは、6つの入力と接続されており、layer0 の出力ノードは 1024 個あるので、1024x6 の行列が取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[285, 227, 432, 459, 774, 502],\n",
       "       [246,   4, 148, 633, 287, 638],\n",
       "       [590, 679, 175, 507, 123, 196],\n",
       "       ...,\n",
       "       [652, 689, 385, 238, 444, 693],\n",
       "       [214, 732, 514,  31,  30, 410],\n",
       "       [548, 372, 435, 273,   8, 598]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_mat = np.array(layer0.get_connection_list())\n",
    "print(connection_mat.shape)\n",
    "connection_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGA化する場合のLUTテーブルを取得する\n",
    "\n",
    "LUT化する場合のテーブルを取得します。<br>\n",
    "6入力のLUTモデルなので $ 2^6 = 64 $ 個のテーブルがあります。<br>\n",
    "モデル内に BatchNormalization 等を含む場合はそれらも加味して最終的にバイナリLUTにする場合に適した値を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ..., False,  True,  True],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [False,  True, False, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False,  True, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lut_mat = np.array(layer0.get_lut_table_list())\n",
    "print(lut_mat.shape)\n",
    "lut_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重み行列を覗いてみる\n",
    "\n",
    "6入力のLUTモデルなので $ 2^6 = 64 $ 個のテーブルがあります。<br>\n",
    "W() にて bb.Tensor 型で取得可能で、numpy() にて ndarray に変換できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5319281 , 0.46951053, 0.5733587 , ..., 0.4717102 , 0.484694  ,\n",
       "        0.5146377 ],\n",
       "       [0.5179902 , 0.48048759, 0.5138343 , ..., 0.61111933, 0.59058636,\n",
       "        0.60133845],\n",
       "       [0.43197432, 0.45865867, 0.46974126, ..., 0.64330673, 0.57856894,\n",
       "        0.60045767],\n",
       "       ...,\n",
       "       [0.52931947, 0.45074382, 0.5597966 , ..., 0.45929053, 0.4534356 ,\n",
       "        0.4742707 ],\n",
       "       [0.50331956, 0.4148569 , 0.47503656, ..., 0.5143429 , 0.55180293,\n",
       "        0.538638  ],\n",
       "       [0.4988206 , 0.499053  , 0.5786278 , ..., 0.37334788, 0.5298596 ,\n",
       "        0.50942224]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = layer0.W().numpy()\n",
    "print(W.shape)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配を覗いてみる\n",
    "\n",
    "同様に dW() でW の勾配が取得できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.9345925e-04, -1.9961728e-03,  3.7461703e-04, ...,\n",
       "        -3.9499150e-06,  2.8121725e-05, -1.0804986e-06],\n",
       "       [-3.1896261e-04, -7.4131065e-05, -1.0632089e-04, ...,\n",
       "        -1.5995120e-06,  7.7564864e-06, -5.3317171e-07],\n",
       "       [-5.1385956e-05, -1.7128652e-05,  7.6244614e-05, ...,\n",
       "         2.2666472e-07,  1.3529657e-06,  4.5098972e-07],\n",
       "       ...,\n",
       "       [ 5.6856754e-04,  2.0029891e-04,  7.1432802e-04, ...,\n",
       "        -7.3252386e-06, -7.3903725e-06, -1.2641797e-06],\n",
       "       [-1.5008255e-04,  1.2325025e-04, -5.0027509e-05, ...,\n",
       "         2.3957109e-06, -3.1364116e-06,  7.9856989e-07],\n",
       "       [-1.5965171e-04, -3.6072830e-04,  6.8319830e-05, ...,\n",
       "        -2.2916847e-06, -1.7123117e-05, -5.1912986e-05]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# そのままだとすべて0なので、1回だけbackward実施\n",
    "for images, labels in loader_test:\n",
    "    x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "    t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "    y_buf = net.forward(x_buf, train=True)\n",
    "    net.backward(loss.calculate(y_buf, t_buf))\n",
    "    break\n",
    "\n",
    "dW = layer0.dW().numpy()\n",
    "print(dW.shape)\n",
    "dW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
