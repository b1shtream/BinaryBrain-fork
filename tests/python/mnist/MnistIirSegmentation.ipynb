{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ff49c-32b3-4d05-88fe-65f6fc5a154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import binarybrain as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48d334-fb03-434b-b164-21389176a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.get_version_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45079310-59c4-4bb5-8b40-a607f37dace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "bb.set_device(0)\n",
    "\n",
    "net_name               = 'mnist_iir_segmentation'\n",
    "data_path              = os.path.join('./data/', net_name)\n",
    "\n",
    "#rtl_sim_path           = '../../verilog/mnist'\n",
    "#rtl_module_name        = 'MnistIirSemanticSegmentation'\n",
    "#output_velilog_file    = os.path.join(data_path, net_name + '.v')\n",
    "#sim_velilog_file       = os.path.join(rtl_sim_path, rtl_module_name + '.v')\n",
    "\n",
    "bin_mode               = True\n",
    "frame_modulation_size  = 3\n",
    "depth_integration_size = 1\n",
    "epochs                 = 8\n",
    "mini_batch_size        = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88499f-e5d8-4718-8139-9a5610371f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並べるタイル数\n",
    "rows=2\n",
    "cols=2\n",
    "\n",
    "img_h = 32\n",
    "img_w = 32\n",
    "\n",
    "def make_teacher_image(gen, rows, cols, margin=0):\n",
    "    source_img  = np.zeros((1, rows*img_h, cols*img_w), dtype=np.float32)\n",
    "    teaching_img = np.zeros((11, rows*img_h, cols*img_w), dtype=np.float32)\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x = col*img_w\n",
    "            y = row*img_h\n",
    "            img, label = gen.__next__()\n",
    "            source_img[0,y:y+img_h,x:x+img_w] = img\n",
    "            teaching_img[label,y:y+img_h,x:x+img_w] = img\n",
    "            teaching_img[10,y:y+img_h,x:x+img_w] = (1.0-img)\n",
    "    teaching_img = (teaching_img > 0.5).astype(np.float32)\n",
    "    \n",
    "    # ランダムに反転\n",
    "    if random.random() > 0.5:\n",
    "        source_img = 1.0 - source_img\n",
    "    \n",
    "    if margin > 0:\n",
    "        return source_img, teaching_img[:,margin:-margin,margin:-margin]\n",
    "    return source_img, teaching_img        \n",
    "\n",
    "def transform_data(dataset, n, rows, cols, margin):\n",
    "    def data_gen():\n",
    "        l = len(dataset)\n",
    "        i = 0\n",
    "        while True:\n",
    "            yield dataset[i%l]\n",
    "            i += 1\n",
    "    \n",
    "    gen = data_gen()\n",
    "    source_imgs = []\n",
    "    teaching_imgs = []\n",
    "    for _ in range(n):\n",
    "        x, t = make_teacher_image(gen, rows, cols, margin)\n",
    "        source_imgs.append(x)\n",
    "        teaching_imgs.append(t)\n",
    "    return source_imgs, teaching_imgs\n",
    "\n",
    "class MyDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, source_imgs, teaching_imgs, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.source_imgs = source_imgs\n",
    "        self.teaching_imgs = teaching_imgs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_img = self.source_imgs[index]\n",
    "        teaching_img = self.teaching_imgs[index]\n",
    "        if self.transforms:\n",
    "            source_img, teaching_img = self.transforms(source_img, teaching_img)\n",
    "        return source_img, teaching_img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((img_w, img_h)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "# dataset\n",
    "dataset_path = './data/'\n",
    "dataset_train = torchvision.datasets.MNIST(root=dataset_path, train=True, transform=transform, download=True)\n",
    "dataset_test  = torchvision.datasets.MNIST(root=dataset_path, train=False, transform=transform, download=True)\n",
    "\n",
    "# 面積の比率で重み作成\n",
    "areas = np.zeros((11))\n",
    "for img, label in dataset_train:\n",
    "    img = img.numpy()\n",
    "    areas[label] += np.mean(img)\n",
    "    areas[10] += np.mean(1.0-img)\n",
    "areas /= len(dataset_train)\n",
    "\n",
    "weight = 1. / areas\n",
    "weight /= np.max(weight)\n",
    "\n",
    "# フィルタ処理用にタイル化する\n",
    "dataset_fname = os.path.join(data_path, 'dataset.pickle')\n",
    "if os.path.exists(dataset_fname):\n",
    "#if False:\n",
    "    with open(dataset_fname, 'rb') as f:\n",
    "        source_imgs_train = pickle.load(f)\n",
    "        teaching_imgs_train = pickle.load(f)\n",
    "        source_imgs_test = pickle.load(f)\n",
    "        teaching_imgs_test = pickle.load(f)\n",
    "else:\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    source_imgs_train, teaching_imgs_train = transform_data(dataset_train, 4096, rows, cols, 0) #29)\n",
    "    source_imgs_test, teaching_imgs_test = transform_data(dataset_test, 128, rows, cols, 0) #, 29)\n",
    "    with open(dataset_fname, 'wb') as f:\n",
    "        pickle.dump(source_imgs_train, f)\n",
    "        pickle.dump(teaching_imgs_train, f)\n",
    "        pickle.dump(source_imgs_test, f)\n",
    "        pickle.dump(teaching_imgs_test, f)\n",
    "\n",
    "my_dataset_train = MyDatasets(source_imgs_train, teaching_imgs_train)\n",
    "my_dataset_test = MyDatasets(source_imgs_test, teaching_imgs_test)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset=my_dataset_train, batch_size=mini_batch_size, shuffle=True)\n",
    "loader_test = torch.utils.data.DataLoader(dataset=my_dataset_test, batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d22819-df3d-45c5-80b2-d69681c72f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ表示確認\n",
    "def plt_data(x, y):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,12,1)\n",
    "    plt.title('sorce')\n",
    "    plt.imshow(x[0], 'gray')\n",
    "    for i in range(11):\n",
    "        plt.subplot(1,12,2+i)\n",
    "        if i < 10:\n",
    "            plt.title('class=%d'%i)\n",
    "            plt.imshow(y[i], 'gray')\n",
    "        else:\n",
    "            plt.title('background')\n",
    "            plt.imshow(y[i], 'gray')\n",
    "    plt.tight_layout()\n",
    "    _ = plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "for source_imgs, teaching_imgs in loader_test:\n",
    "    print(source_imgs[0].shape)\n",
    "    print(teaching_imgs[0].shape)\n",
    "    for i in range(min(mini_batch_size, 4)):\n",
    "        plt_data(source_imgs[i], teaching_imgs[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46eba4-41b0-4345-b789-4ffcb3ed1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バイナリ時は BIT型を使えばメモリ削減可能\n",
    "bin_dtype = bb.DType.BIT if bin_mode else bb.DType.FP32\n",
    "\n",
    "def create_dense_affine(name, output_ch, fw_dtype=bin_dtype):\n",
    "    \"\"\"バイナリ化したDenseAffine層生成\"\"\"\n",
    "    return bb.Sequential([\n",
    "                bb.DenseAffine([output_ch, 1, 1], name=(name + '_dense_affine')),\n",
    "                bb.BatchNormalization(name=(name + '_dense_bn')),\n",
    "                bb.Binarize(name=(name + '_dense_act'), bin_dtype=fw_dtype),\n",
    "            ])\n",
    "\n",
    "def create_dense_conv(name, output_ch, filter_size=(3, 3), padding='same', fw_dtype=bin_dtype):\n",
    "    \"\"\"バイナリ化したDenseConv層生成\"\"\"\n",
    "    return bb.Convolution2d(\n",
    "                create_dense_affine(name, output_ch, fw_dtype),\n",
    "                filter_size=filter_size,\n",
    "                padding=padding,\n",
    "                name=(name + '_dense_conv'),\n",
    "                fw_dtype=fw_dtype)\n",
    "\n",
    "def create_conv_block(name, output_ch=64):\n",
    "    return bb.Sequential([\n",
    "                create_dense_conv(name + '_cnv0', output_ch),\n",
    "                create_dense_conv(name + '_cnv1', output_ch),\n",
    "            ])\n",
    "\n",
    "\n",
    "class ScaledNetwork(bb.Sequential):\n",
    "    def __init__(self, name, hidden_ch=32, output_ch=32, top=False):\n",
    "        self.hidden_ch = hidden_ch\n",
    "        self.output_ch = output_ch\n",
    "        self.top       = top\n",
    "        \n",
    "        self.up   = bb.UpSampling((2, 2), fw_dtype=bin_dtype)    \n",
    "        self.pool = bb.MaxPooling((2, 2), fw_dtype=bin_dtype)\n",
    "        self.cat0 = bb.Concatenate()\n",
    "        self.cat1 = bb.Concatenate()\n",
    "        self.cnv0 = create_conv_block(name + '_cnv0', self.hidden_ch)\n",
    "        self.cnv1 = create_conv_block(name + '_cnv1', self.output_ch)\n",
    "        super(ScaledNetwork, self).__init__([self.up, self.pool, self.cat0, self.cat1, self.cnv0, self.cnv1])\n",
    "    \n",
    "    def set_input_shape(self, x0_shape, x1_shape, u_shape):\n",
    "        x1_shape = self.up.set_input_shape(x1_shape)\n",
    "        x_shape = self.cat0.set_input_shape([x0_shape, x1_shape])\n",
    "        x_shape = self.cnv0.set_input_shape(x_shape)\n",
    "        v_shape = self.pool.set_input_shape(x_shape)\n",
    "        if not self.top:\n",
    "            x_shape = self.cat1.set_input_shape([u_shape, x_shape])\n",
    "        y_shape = self.cnv1.set_input_shape(x_shape)\n",
    "        return y_shape, v_shape\n",
    "    \n",
    "    def forward(self, x0, x1, u, train=True):\n",
    "        x1 = self.up.forward(x1, train=train)\n",
    "        x = self.cat0.forward([x0, x1])\n",
    "        x = self.cnv0.forward(x, train=train)        \n",
    "        v = self.pool.forward(x, train=train)\n",
    "        if not self.top:\n",
    "            x = self.cat1.forward([u, x])\n",
    "        y = self.cnv1.forward(x, train=train)\n",
    "        return y, v\n",
    "    \n",
    "    def backward(self, dy, dv):\n",
    "        dy = self.cnv1.backward(dy)\n",
    "        if not self.top:\n",
    "            du, dy0 = self.cat1.backward([dy])\n",
    "        else:\n",
    "            du, dy0 = None, dy\n",
    "        dy1 = self.pool.backward(dv)\n",
    "        dy = self.cnv0.backward(dy0 + dy1)\n",
    "        dx0, dx1 = self.cat0.backward([dy])\n",
    "        dx1 = self.up.backward(dx1)\n",
    "        return dx0, dx1, du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb36c0-aa20-4fee-8465-57a202352ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MipmapNetwork(bb.Sequential):\n",
    "    def __init__(self, loop=6, ch=32, depth=3):\n",
    "        self.loop    = loop\n",
    "        self.depth   = depth\n",
    "        self.ch      = ch\n",
    "        self.shape   = None\n",
    "        self.up      = bb.UpSampling((2, 2))\n",
    "        self.m_net = ScaledNetwork('main', ch, 11, top=True)\n",
    "        self.s_net = ScaledNetwork('sub', ch, ch)\n",
    "        super(MipmapNetwork, self).__init__([self.m_net, self.s_net])\n",
    "    \n",
    "    def set_input_shape(self, shape):\n",
    "        self.shape = copy.copy(shape)\n",
    "        \n",
    "        x0_shape = copy.copy(shape)\n",
    "        x1_shape = copy.copy(shape)\n",
    "        x1_shape[0] = self.ch\n",
    "        x1_shape[1] //= 2\n",
    "        x1_shape[2] //= 2\n",
    "        u_shape = copy.copy(shape)\n",
    "        y_shape, v_shape = self.m_net.set_input_shape(x0_shape, x1_shape, u_shape)\n",
    "        \n",
    "        x0_shape = copy.copy(x1_shape)\n",
    "        x0_shape[0] = self.ch\n",
    "        x1_shape = copy.copy(x1_shape)\n",
    "        x1_shape[0] = self.ch\n",
    "        x1_shape[1] //= 2\n",
    "        x1_shape[2] //= 2\n",
    "        self.s_net.set_input_shape(x0_shape, x1_shape, v_shape)\n",
    "        \n",
    "        return y_shape\n",
    "    \n",
    "    def make_mipmap(self, frame_size, dtype=bb.DType.FP32):\n",
    "        h = self.shape[1]\n",
    "        w = self.shape[2]\n",
    "        mipmap = []\n",
    "        for i in range(self.depth+1):\n",
    "            h //= 2\n",
    "            w //= 2\n",
    "            buf = bb.FrameBuffer(frame_size, (self.ch, h, w), dtype=dtype)\n",
    "            buf.fill_zero()\n",
    "            mipmap.append(buf)\n",
    "        return mipmap\n",
    "    \n",
    "    def forward(self, x, train=True):\n",
    "        self.m_net.clear()\n",
    "        self.s_net.clear()\n",
    "        \n",
    "        frame_size = x.get_frame_size()\n",
    "        \n",
    "#       u_shape = x.get_node_shape()\n",
    "#       u_shape[0] = self.ch\n",
    "#       u = bb.FrameBuffer(frame_size, u_shape, dtype=bb.DType.BIT)\n",
    "#       u.fill_zero()\n",
    "        \n",
    "        mipmap = self.make_mipmap(frame_size, dtype=bin_dtype)        \n",
    "        for _ in range(self.loop):\n",
    "            y, v = self.m_net.forward(x, mipmap[0], None, train=train)\n",
    "            for i in range(self.depth):\n",
    "                mipmap[i], v = self.s_net.forward(mipmap[i], mipmap[i+1], v)\n",
    "        \n",
    "        self.v_shape = v.get_node_shape()\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        frame_size = dy.get_frame_size()\n",
    "        dv = bb.FrameBuffer(frame_size, self.v_shape, dtype=bb.DType.FP32)\n",
    "        dv.fill_zero()\n",
    "        \n",
    "        mipmap = self.make_mipmap(frame_size, dtype=bb.DType.FP32)\n",
    "        for _ in range(self.loop):\n",
    "            du = dv\n",
    "            for i in reversed(range(self.depth)):\n",
    "                dx0, dx1, du = self.s_net.backward(mipmap[i], du)\n",
    "                mipmap[i]   += dx0\n",
    "                mipmap[i+1] += dx1\n",
    "            dx0, dx1, du = self.m_net.backward(dy, du)\n",
    "            mipmap[0] += dx1\n",
    "        return dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97021ff-6fb7-4aa4-8a3d-f74721ec0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MipmapNetwork()\n",
    "net.set_input_shape([1, img_h*rows, img_w*cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd23904-75fc-431d-b3b9-ebc7eec0223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning\n",
    "if True:\n",
    "    loss      = bb.LossSoftmaxCrossEntropy()\n",
    "    metrics   = bb.MetricsCategoricalAccuracy()\n",
    "    optimizer = bb.OptimizerAdam()\n",
    "\n",
    "    optimizer.set_variables(net.get_parameters(), net.get_gradients())\n",
    "\n",
    "    real2bin = bb.RealToBinary(frame_modulation_size=frame_modulation_size, bin_dtype=bin_dtype)\n",
    "    bin2real = bb.BinaryToReal(frame_integration_size=frame_modulation_size, bin_dtype=bin_dtype)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # learning\n",
    "        loss.clear()\n",
    "        metrics.clear()\n",
    "        with tqdm(loader_train) as tqdm_loadr:\n",
    "            for x_imgs, t_imgs in tqdm_loadr:\n",
    "                x_buf = bb.FrameBuffer.from_numpy(np.array(x_imgs).astype(np.float32))\n",
    "                t_buf = bb.FrameBuffer.from_numpy(np.array(t_imgs).astype(np.float32))\n",
    "\n",
    "                x_buf = real2bin.forward(x_buf, train=True)\n",
    "#               print('fw:', x_buf.get_frame_size(), x_buf.get_node_shape())\n",
    "                y_buf = net.forward(x_buf, train=True)\n",
    "#               print('fw:', y_buf.get_frame_size(), y_buf.get_node_shape())\n",
    "                y_buf = bin2real.forward(y_buf, train=True)\n",
    "\n",
    "                dy_buf = loss.calculate(y_buf, t_buf)\n",
    "                metrics.calculate(y_buf, t_buf)\n",
    "                \n",
    "#               print('bw:', dy_buf.get_frame_size(), dy_buf.get_node_shape())\n",
    "                dy_buf = bin2real.backward(dy_buf)\n",
    "                net.backward(dy_buf)\n",
    "                optimizer.update()\n",
    "\n",
    "                tqdm_loadr.set_postfix(loss=loss.get(), acc=metrics.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13279644-1517-43d7-be18-3200accac8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5b91f-39d5-4e8a-b033-effd0d57ca81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c863d-0d40-4534-9d3b-a35e9012d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bb.FrameBuffer(48, (1, 32*2, 32*2), dtype=bb.DType.BIT)\n",
    "y_shape = net.set_input_shape(x.get_node_shape())\n",
    "print(x.get_node_shape())\n",
    "print(y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894967c-cce5-4f7a-9818-f5e020ff4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(x.get_frame_size(), x.get_node_shape())\n",
    "    y = net.forward(x, train=True)\n",
    "    print(y.get_frame_size(), y.get_node_shape())\n",
    "\n",
    "    dy = bb.FrameBuffer(y.get_frame_size(), y.get_node_shape(), dtype=bb.DType.FP32)\n",
    "\n",
    "    print(dy.get_frame_size(), dy.get_node_shape())\n",
    "    dx = net.backward(dy)\n",
    "    print(dx.get_frame_size(), dx.get_node_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c65b43-c948-46f4-b2ba-19f91b217557",
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35ebc6-09a3-4f23-a15d-150e291184a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MipmapNetwork(bb.Sequential):\n",
    "    def __init__(self, loop=8, ch=32, depth=4):\n",
    "        self.loop    = loop\n",
    "        self.depth   = depth\n",
    "        self.ch      = ch\n",
    "        self.shape   = None\n",
    "        self.up      = bb.UpSampling((2, 2))\n",
    "        self.m_net = ScaledNetwork('main', ch, 11)\n",
    "        self.s_net = ScaledNetwork('sub', ch, ch)\n",
    "        super(MipmapNetwork, self).__init__([self.m_net, self.s_net])\n",
    "    \n",
    "    def set_input_shape(self, u_shape):\n",
    "        x0_shape = copy.copy(u_shape)\n",
    "        x0_shape[0] = self.ch\n",
    "        x1_shape = copy.copy(u_shape)\n",
    "        x1_shape[0] = self.ch\n",
    "        x1_shape[1] //= 2\n",
    "        x1_shape[2] //= 2\n",
    "        self.shape = x0_shape\n",
    "        self.s_net.set_input_shape(x0_shape, x1_shape, u_shape)\n",
    "        v_shape = copy.copy(u_shape)\n",
    "        for i in range(self.depth):\n",
    "            v_shape[1] //= 2\n",
    "            v_shape[2] //= 2\n",
    "        return v_shape\n",
    "    \n",
    "    def make_mipmap(self, frame_size, dtype=bb.DType.FP32):\n",
    "        h = self.shape[1]\n",
    "        w = self.shape[2]\n",
    "        mipmap = []\n",
    "        for i in range(self.depth+1):\n",
    "            buf = bb.FrameBuffer(frame_size, (self.ch, h, w), dtype=dtype)\n",
    "            buf.fill_zero()\n",
    "            mipmap.append(buf)\n",
    "            h //= 2\n",
    "            w //= 2\n",
    "        return mipmap\n",
    "    \n",
    "    def forward(self, u, train=True):\n",
    "        self.m_net.clear()\n",
    "        self.s_net.clear()\n",
    "        mipmap = self.make_mipmap(u.get_frame_size(), dtype=bin_dtype)\n",
    "        for _ in range(self.loop):\n",
    "            u0 = u\n",
    "            for i in range(self.depth):\n",
    "                mipmap[i], u0 = self.s_net.forward(mipmap[i], mipmap[i+1], u0)\n",
    "        return u0\n",
    "    \n",
    "    def backward(self, dv):\n",
    "        mipmap = self.make_mipmap(dv.get_frame_size())\n",
    "        for _ in range(self.loop):\n",
    "            dv0 = dv\n",
    "            for i in reversed(range(self.depth)):\n",
    "                dx0, dx1, dv0 = self.sub_net.backward(mipmap[i], dv0)\n",
    "                mipmap[i]   += dx0\n",
    "                mipmap[i+1] += dx1\n",
    "        return dv0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cb66b-bd85-442a-87fd-4e801f249002",
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_net = MipmapNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ec2a4-5b7a-4af9-8704-b23ac1613bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = bb.FrameBuffer(32, (32, 32*3//2, 32*3//2), dtype=bb.DType.BIT)\n",
    "mip_net.set_input_shape(u.get_node_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096ffdb-4323-4ade-a7eb-361738feedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mip_net.sub_net.cnv0[0].get_info())\n",
    "#print(mip_net.sub_net.cnv1[0].get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96794dcd-30c3-4a9d-b49c-ce7c726027ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = mip_net.forward(u)\n",
    "print(v.get_node_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20148093-ca3e-467c-badd-985aa7abdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = bb.FrameBuffer(v.get_frame_size(), v.get_node_shape(), dtype=bb.DType.FP32)\n",
    "du = mip_net.backward(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06d852-d102-4a5b-826e-a6759457eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(du.get_frame_size(), du.get_node_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59794bac-6748-4cac-9c54-385b3a827ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da6b25-7d4b-4899-9070-158fea636802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationIirNetwork(bb.Sequential):\n",
    "    \"\"\"セグメンテーション＋分類ネットワーク\"\"\"\n",
    "    def __init__(self):\n",
    "        self.ch    = 32\n",
    "        self.depth = 4\n",
    "\n",
    "        self.real2bin = bb.RealToBinary(frame_modulation_size=frame_modulation_size, bin_dtype=bin_dtype)\n",
    "        self.bin2real = bb.BinaryToReal(frame_integration_size=frame_modulation_size, bin_dtype=bin_dtype)\n",
    "        \n",
    "        self.pool  = bb.MaxPooling((2, 2))\n",
    "        self.up    = bb.UpSampling((2, 2))\n",
    "        \n",
    "        # 等倍用\n",
    "        self.cnv0 = create_conv_block('m_cnv0', self.ch)\n",
    "        self.cnv1 = create_conv_block('m_cnv1', self.ch)\n",
    "        \n",
    "        # ミップマップ用\n",
    "        self.mipmap = MipmapNetwork(loop=8, ch=32, depth=4)\n",
    "        \n",
    "        super(SemanticSegmentationIirNetwork, self).__init__([self.real2bin, self.bin2real, self.pool, self.up, self.cnv0, self.cnv1, self.mipmap])\n",
    "        \n",
    "    def set_input_shape(self, shape):\n",
    "        shape = self.real2bin.set_input_shape(shape)\n",
    "        shape[0] += 1\n",
    "        self.m_cnv0.set_input_shape(shape)\n",
    "        \n",
    "        shape = self.net_cnv.set_input_shape(shape)\n",
    "        shape = self.bin2real.set_input_shape(shape)\n",
    "        return shape\n",
    "    \n",
    "    def forward(self, x, train=True):\n",
    "        shape = x.get_node_shape()\n",
    "        n = x.get_frame_size()\n",
    "        c = shape[0]\n",
    "        h = shape[1]\n",
    "        w = shape[2]\n",
    "        \n",
    "        # mipmap作成\n",
    "        mipmap = []\n",
    "        for i in range(self.depth):\n",
    "            h //= 2\n",
    "            w //= 2\n",
    "            buf = bb.FrameBuffer(n, (self.ch, h, w), dtype=bin_dtype)\n",
    "#           buf.fill_zero()\n",
    "            mipmap.append(buf)\n",
    "        \n",
    "        # フィルタ効果が出るまで何回かループする\n",
    "        x = self.real2bin.forward(x, train=train)\n",
    "        for loop in range(3):\n",
    "            # ミップマップの一番大きいものと concatして推論\n",
    "            y = self.m_cnv0.forward(buf_concat(x, self.up.forward(mipmap[0], train=train)), train=train)\n",
    "            x0 = self.pool.forward(y, train=train)\n",
    "            y = self.m_cnv1.forward(y, train=train)\n",
    "            \n",
    "            # ミップマップの上下階層をconcatしてそれぞれ同じネットで推論\n",
    "            for i in range(self.depth):\n",
    "                if i+1 < self.depth:\n",
    "                    x1 = self.s_cnv0.forward(buf_concat(mipmap[i], self.up.forward(mipmap[i+1], train=train)), train=train)\n",
    "                else:\n",
    "                    x1 = self.s_cnv0.forward(buf_concat(mipmap[i], mipmap[i]), train=train) # 最下層\n",
    "                mipmap[i] = self.s_cnv1.forward(buf_concat(x1, x0), train=train)\n",
    "                x0 = self.pool.forward(x1, train=train)\n",
    "        \n",
    "        y = self.bin2real.forward(y, train=train)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        shape = dy.get_node_shape()\n",
    "        n = dy.get_frame_size()\n",
    "        c = shape[0]\n",
    "        h = shape[1]\n",
    "        w = shape[2]\n",
    "        \n",
    "        # mipmap作成\n",
    "        mipmap = []\n",
    "        for i in range(self.depth):\n",
    "            h //= 2\n",
    "            w //= 2\n",
    "            buf = bb.FrameBuffer(n, (self.ch, h, w), dtype=bin_dtype)\n",
    "#           buf.fill_zero()\n",
    "            mipmap.append(buf)\n",
    "        dy = self.bin2real.backward(dy)\n",
    "        \n",
    "        dy0 = bb.FrameBuffer(n, (self.ch, h, w))\n",
    "#       dy0.fill_zero()\n",
    "        for loop in reversed(range(3)):\n",
    "            for i in reversed(self.depth):\n",
    "                dy0 = self.pool.backward(dy0)\n",
    "                dy0, dy1 = buf_split(self.s_cnv1.backward(mipmap[i]), self.ch//2)\n",
    "                    \n",
    "                    buf_concat(x1, x0), train=train)\n",
    "                \n",
    "      \n",
    "        return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42a3a4-a182-4ee4-b289-bf7524116c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
