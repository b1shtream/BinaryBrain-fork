{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIRフィルタ的なことを試してみるテスト\n",
    "\n",
    "結果を MaxPooling で縮小して次のフレームで使う実験をしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import binarybrain as bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット\n",
    "\n",
    "データセットの準備には torchvision を使います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "net_name              = 'MnistIirFilterTestt'\n",
    "data_path             = os.path.join('./data/', net_name)\n",
    "\n",
    "bin_mode              = False\n",
    "frame_modulation_size = 7\n",
    "epochs                = 4\n",
    "mini_batch_size       = 64\n",
    "\n",
    "# dataset\n",
    "dataset_path = './data/'\n",
    "dataset_train = torchvision.datasets.MNIST(root=dataset_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "dataset_test  = torchvision.datasets.MNIST(root=dataset_path, train=False, transform=transforms.ToTensor(), download=True)\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=mini_batch_size, shuffle=True, num_workers=2)\n",
    "loader_test  = torch.utils.data.DataLoader(dataset=dataset_test,  batch_size=mini_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワーク構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バイナリ時は BIT型を使えばメモリ削減可能\n",
    "bin_dtype = bb.DType.BIT if bin_mode else bb.DType.FP32\n",
    "\n",
    "def create_cnv(output_ch, filter_size=(3, 3), padding='same', fw_dtype=bin_dtype):\n",
    "    return bb.Convolution2d(\n",
    "                bb.Sequential([\n",
    "                    bb.DenseAffine([output_ch, 1, 1]),\n",
    "                    bb.ReLU(),\n",
    "                ]),\n",
    "                filter_size=filter_size,\n",
    "                padding=padding,\n",
    "                fw_dtype=fw_dtype)\n",
    "\n",
    "def create_fc(output_ch, fw_dtype=bin_dtype):\n",
    "    return bb.Convolution2d(\n",
    "                bb.Sequential([\n",
    "                    bb.DenseAffine([output_ch, 1, 1]),\n",
    "                ]),\n",
    "                filter_size=(1, 1),\n",
    "                fw_dtype=fw_dtype)\n",
    "\n",
    "class MyNetwork(bb.Sequential):\n",
    "    def __init__(self):\n",
    "        self.N = 4\n",
    "#       self.r2b = bb.RealToBinary(frame_modulation_size=frame_modulation_size, bin_dtype=bin_dtype)\n",
    "#       self.b2r = bb.BinaryToReal(frame_integration_size=frame_modulation_size, bin_dtype=bin_dtype)\n",
    "        \n",
    "        self.cnvs0 = bb.Sequential()\n",
    "        self.cnvs1 = bb.Sequential()\n",
    "        self.fcs   = bb.Sequential()\n",
    "        self.upss  = bb.Sequential()\n",
    "        self.pols  = bb.Sequential()\n",
    "        for _ in range(self.N):\n",
    "            self.cnvs0.append(create_cnv(64))\n",
    "            self.cnvs1.append(create_cnv(64))\n",
    "            self.fcs.append(create_fc(10))     \n",
    "            self.upss.append(bb.UpSampling((2, 2)))\n",
    "            self.pols.append(bb.MaxPooling((2, 2)))\n",
    "        super(MyNetwork, self).__init__([self.cnvs0, self.cnvs1, self.fcs, self.upss, self.pols])\n",
    "    \n",
    "    def set_input_shape(self, shape):\n",
    "        for i in range(self.N):\n",
    "            shape1 = self.cnvs0[i].set_input_shape(shape)\n",
    "        for i in range(self.N):\n",
    "            shape2 = self.cnvs1[i].set_input_shape(shape1)\n",
    "        for i in range(self.N):\n",
    "            self.fcs[i].set_input_shape(shape2)\n",
    "        for i in range(self.N):\n",
    "            shape3 = self.pols[i].set_input_shape(shape2)\n",
    "        for i in range(self.N):\n",
    "            self.upss[i].set_input_shape(shape3)\n",
    "    \n",
    "    def param_copy(self):\n",
    "        for i in range(1, self.N):\n",
    "            W = self.cnvs0[i][1][0].W()\n",
    "            b = self.cnvs0[i][1][0].b()\n",
    "            W *= 0; W += self.cnvs0[0][1][0].W()\n",
    "            b *= 0; b += self.cnvs0[0][1][0].b()\n",
    "            \n",
    "            W = self.cnvs1[i][1][0].W()\n",
    "            b = self.cnvs1[i][1][0].b()\n",
    "            W *= 0; W += self.cnvs1[0][1][0].W()\n",
    "            b *= 0; b += self.cnvs1[0][1][0].b()\n",
    "            \n",
    "            W = self.fcs[i][1][0].W()\n",
    "            b = self.fcs[i][1][0].b()\n",
    "            W *= 0; W += self.fcs[0][1][0].W()\n",
    "            b *= 0; b += self.fcs[0][1][0].b()\n",
    "            \n",
    "    def grad_marge(self):\n",
    "        dW0 = self.cnvs0[0][1][0].dW()\n",
    "        db0 = self.cnvs0[0][1][0].db()\n",
    "        dW1 = self.cnvs1[0][1][0].dW()\n",
    "        db1 = self.cnvs1[0][1][0].db()\n",
    "        dW2 = self.fcs[0][1][0].dW()\n",
    "        db2 = self.fcs[0][1][0].db()\n",
    "        for i in range(1, self.N):\n",
    "            dW0 += self.cnvs0[i][1][0].dW()\n",
    "            db0 += self.cnvs0[i][1][0].db()\n",
    "            dW1 += self.cnvs1[i][1][0].dW()\n",
    "            db1 += self.cnvs1[i][1][0].db()\n",
    "            dW2 += self.fcs[i][1][0].dW()\n",
    "            db2 += self.fcs[i][1][0].db()\n",
    "    \n",
    "    def forward(self, x_buf, train=True):\n",
    "        x = x_buf.numpy()\n",
    "        px_buf = bb.FrameBuffer.from_numpy(np.zeros((x_buf.get_frame_size(), 64, 14, 14), dtype=np.float32))\n",
    "        \n",
    "        y_bufs = []\n",
    "        for i in range(self.N):\n",
    "            # 前のフレームの出力と結合\n",
    "            px_buf = self.upss[i].forward(px_buf, train=train)\n",
    "            px = px_buf.numpy()\n",
    "            x_buf = bb.FrameBuffer.from_numpy(np.concatenate((x, px), 1))\n",
    "            \n",
    "            # forward\n",
    "            x_buf = self.cnvs0[i].forward(x_buf, train=train)\n",
    "            x_buf = self.cnvs1[i].forward(x_buf, train=train)\n",
    "            \n",
    "            y_buf = self.fcs[i].forward(x_buf, train=train)\n",
    "            \n",
    "            # 出力の1つとして追加\n",
    "            y_bufs.append(y_buf)\n",
    "            \n",
    "            px_buf = self.pols[i].forward(x_buf, train=train)\n",
    "        \n",
    "        return y_bufs\n",
    "    \n",
    "    def backward(self, dy_bufs):\n",
    "        pdy_buf = bb.FrameBuffer.from_numpy(np.zeros((dy_bufs[0].get_frame_size(), 64, 14, 14), dtype=np.float32))\n",
    "        for i in reversed(range(self.N)):\n",
    "            pdy_buf = self.pols[i].backward(pdy_buf)\n",
    "            dy_buf = self.fcs[i].backward(dy_bufs[i])\n",
    "            dx_buf = self.cnvs1[i].backward(dy_buf + pdy_buf)\n",
    "            dx_buf = self.cnvs0[i].backward(dx_buf)\n",
    "            dx = dx_buf.numpy()[:,1:]\n",
    "            pdy_buf = self.upss[i].backward(bb.FrameBuffer.from_numpy(dx))\n",
    "        \n",
    "    \n",
    "net = MyNetwork()\n",
    "net.set_input_shape([64+1, 28, 28])\n",
    "net.param_copy()\n",
    "net.grad_marge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb.load_networks(data_path, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(net.N):\n",
    "    losses.append(bb.LossSoftmaxCrossEntropy())\n",
    "\n",
    "optimizer = bb.OptimizerAdam()\n",
    "metrics   = bb.MetricsCategoricalAccuracy()\n",
    "\n",
    "parameters = bb.Variables()\n",
    "parameters.append(net.cnvs0[0].get_parameters())\n",
    "parameters.append(net.cnvs1[0].get_parameters())\n",
    "parameters.append(net.fcs[0].get_parameters())\n",
    "gradients = bb.Variables()\n",
    "gradients.append(net.cnvs0[0].get_gradients())\n",
    "gradients.append(net.cnvs1[0].get_gradients())\n",
    "gradients.append(net.fcs[0].get_gradients())\n",
    "optimizer.set_variables(parameters, gradients)\n",
    "\n",
    "epochs = 32\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(loader_train) as tq:\n",
    "        for images, labels in tq:\n",
    "            x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "            t = np.zeros((len(labels), 10, 28, 28), dtype=np.float32)\n",
    "            for i in range(len(labels)):\n",
    "                t[i][labels[i]][13:15,13:15] += 1  # 中央付近のピクセルだけで評価\n",
    "            t_buf = bb.FrameBuffer.from_numpy(t)\n",
    "\n",
    "            net.param_copy()\n",
    "            y_bufs = net.forward(x_buf, train=True)\n",
    "\n",
    "            dy_bufs = []\n",
    "            for i in range(net.N):\n",
    "                dy_buf = losses[i].calculate(y_bufs[i], t_buf)\n",
    "                dy_bufs.append(dy_buf)\n",
    "            \n",
    "            metrics.calculate(y_bufs[net.N-1], t_buf)\n",
    "            \n",
    "            net.backward(dy_bufs)\n",
    "            net.grad_marge()\n",
    "\n",
    "            optimizer.update()\n",
    "                        \n",
    "            loss = 0\n",
    "            for i in range(net.N):\n",
    "                loss += losses[i].get()\n",
    "                \n",
    "            tq.set_postfix(loss=loss, metrics=metrics.get())\n",
    "    bb.save_networks(data_path, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.save_networks(data_path, net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
