{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微分可能LUTモデルによるMNIST学習\n",
    "\n",
    "Stochasticモデルに BatchNormalization や Binarize(backward時はHard-Tanh)を加えることで、より一般的なデータに対してLUT回路学習を行います。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import binarybrain as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb.set_host_only(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GT 1030\n"
     ]
    }
   ],
   "source": [
    "print(bb.get_device_name(1))\n",
    "bb.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "異なる閾値で2値化した画像でフレーム数を水増ししながら学習させます。この水増しをバイナリ変調と呼んでいます。\n",
    "\n",
    "ここではフレーム方向の水増し量を frame_modulation_size で指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "data_path             = './data/'\n",
    "net_name              = 'MnistDifferentiableLutSimple'\n",
    "data_path             = os.path.join('./data/', net_name)\n",
    "rtl_sim_path          = '../../verilog/mnist/tb_mnist_lut_simple'\n",
    "rtl_module_name       = 'MnistLutSimple'\n",
    "output_velilog_file   = os.path.join(data_path, net_name + '.v')\n",
    "sim_velilog_file      = os.path.join(rtl_sim_path, rtl_module_name + '.v')\n",
    "\n",
    "epochs                = 4\n",
    "mini_batch_size       = 64*4\n",
    "frame_modulation_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(rtl_sim_path, exist_ok=True)\n",
    "os.makedirs(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットは PyTorch の torchvision を使います。ミニバッチのサイズも DataLoader で指定しています。\n",
    "BinaryBrainではミニバッチをフレーム数として FrameBufferオブジェクトで扱います。\n",
    "バイナリ変調で計算中にフレーム数が変わるためデータセットの準備観点でのミニバッチと呼び分けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset_path = './data/'\n",
    "dataset_train = torchvision.datasets.MNIST(root=dataset_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "dataset_test  = torchvision.datasets.MNIST(root=dataset_path, train=False, transform=transforms.ToTensor(), download=True)\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=mini_batch_size, shuffle=True, num_workers=2)\n",
    "loader_test  = torch.utils.data.DataLoader(dataset=dataset_test,  batch_size=mini_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの構築\n",
    "\n",
    "DifferentiableLut に特に何もオプションをつけなければOKです。<br>\n",
    "バイナリ変調を施すためにネットの前後に RealToBinary層とBinaryToReal層を入れています。<br>\n",
    "send_command で \"binary true\" を送ることで、DifferentiableLut の内部の重み係数が 0.0-1.0 の間に拘束されます。\n",
    "\n",
    "接続数がLUTの物理構成に合わせて、1ノード当たり6個なので層間で6倍以上ノード数が違うと接続されないノードが発生するので、注意してネットワーク設計が必要です。\n",
    "最終段は各クラス7個の結果を出して Reduce で足し合わせています。こうすることで若干の改善がみられるとともに、加算結果が INT3 相当になるために若干尤度を数値的に見ることができるようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "net = bb.Sequential([\n",
    "#           bb.RealToBinary(),\n",
    "            bb.Binarize(binary_th=0.5, binary_low=0.0, binary_high=1.0),\n",
    "            bb.DifferentiableLut([256]),\n",
    "            bb.DifferentiableLut([256]),\n",
    "            bb.DifferentiableLut([128]),\n",
    "            bb.DifferentiableLut([128]),\n",
    "            bb.DifferentiableLut([10]),\n",
    "#           bb.DenseAffineQuantize([10]),\n",
    "#            bb.DenseAffineQuantize([10], quantize = True,\n",
    "#                    weight_bits = 16, output_bits = 16, input_bits = 16,\n",
    "#                    weight_scale = 1.0/(1<<8), output_scale = 1.0/(1<<8), input_scale = 0),\n",
    "        ])\n",
    "net.set_input_shape([1, 28, 28])\n",
    "\n",
    "net.send_command(\"binary true\")\n",
    "\n",
    "loss      = bb.LossSoftmaxCrossEntropy()\n",
    "metrics   = bb.MetricsCategoricalAccuracy()\n",
    "optimizer = bb.OptimizerAdam(learning_rate=0.0001)\n",
    "#optimizer = bb.OptimizerSgd()\n",
    "#optimizer = bb.OptimizerAdaGrad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "[Sequential] \n",
      " input  shape : [1, 28, 28] output shape : [10]\n",
      "  --------------------------------------------------------------------\n",
      "  [Binarize] \n",
      "   input  shape : {1, 28, 28} output shape : {1, 28, 28}\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {1, 28, 28} output shape : {256}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {256} output shape : {256}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {256} output shape : {128}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {128} output shape : {128}\n",
      "   binary : 1   batch_norm : 1\n",
      "  --------------------------------------------------------------------\n",
      "  [DifferentiableLut6] \n",
      "   input  shape : {128} output shape : {10}\n",
      "   binary : 1   batch_norm : 1\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実施\n",
    "\n",
    "load_networks/save_networks で途中結果を保存/復帰可能できます。ネットワークの構造が変わると正常に読み込めなくなるので注意ください。\n",
    "(その場合は新しいネットをsave_networksするまで一度load_networks をコメントアウトください)\n",
    "\n",
    "tqdm などを使うと学習過程のプログレス表示ができて便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af4c578a7904669afae885d015dc5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013892f6fbcb4b43a680197d35168dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9163c1934fac4112b07d20ccc6890713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783c5c90e6ee4fffa0771e776f8d9f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bb.load_networks(data_path, net)\n",
    "\n",
    "#optimizer = bb.OptimizerAdam(learning_rate=0.0001)\n",
    "optimizer.set_variables(net.get_parameters(), net.get_gradients())\n",
    "\n",
    "list_W = [[]]*6\n",
    "\n",
    "# learning\n",
    "for epoch in range(epochs):\n",
    "    # learning\n",
    "    loss.clear()\n",
    "    metrics.clear()\n",
    "    with tqdm(loader_train) as t:\n",
    "        for images, labels in t:\n",
    "#        for images, labels in loader_train:\n",
    "            x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "            t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "\n",
    "            y_buf = net.forward(x_buf, train=True)\n",
    "\n",
    "            dy_buf = loss.calculate(y_buf, t_buf)\n",
    "            metrics.calculate(y_buf, t_buf)\n",
    "\n",
    "            net.backward(dy_buf)\n",
    "\n",
    "            optimizer.update()\n",
    "        \n",
    "            t.set_postfix(loss=loss.get(), acc=metrics.get())\n",
    "\n",
    "    # test\n",
    "    loss.clear()\n",
    "    metrics.clear()\n",
    "    for images, labels in loader_test:\n",
    "        x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "        t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "\n",
    "        y_buf = net.forward(x_buf, train=False)\n",
    "\n",
    "        loss.calculate(y_buf, t_buf)\n",
    "        metrics.calculate(y_buf, t_buf)\n",
    "\n",
    "#    for i in range(6, 6):\n",
    "#        net[i].dump('test%d_.bb_net'%i)\n",
    "#        net[i].load('test%d_.bb_net'%i)\n",
    "    bb.save_networks(data_path, net)\n",
    "    bb.load_networks(data_path, net)\n",
    "    optimizer.set_variables(net.get_parameters(), net.get_gradients())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1366586092.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    ------------\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('err_W_.pickle', 'rb') as f:\n",
    "    pickle.dump(list_W, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('org_W_.pickle', 'rb') as f:\n",
    "    org_W = pickle.load(f)\n",
    "with open('err_W_.pickle', 'rb') as f:\n",
    "    err_W = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_W[0][0] == org_W[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_W[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_W[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_W[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_layer = net[1]\n",
    "org_layer.dump('test_.bin')\n",
    "new_layer = bb.object_load('test_.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_W  = org_layer.W().numpy()\n",
    "org_dW = org_layer.dW().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_layer.get_core().equality_check(new_layer.get_core())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((org_layer.W().numpy() == new_layer.W().numpy()).all())\n",
    "print((org_layer.dW().numpy() == new_layer.dW().numpy()).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(org_layer.W().numpy() == new_layer.W().numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGA用Verilog出力\n",
    "\n",
    "最後に学習したネットワークを Verilog 出力します。\n",
    "MNISTのサイズである 28x28=784bit の入力を 10bit の分類をして出力するだけのシンプルなモジュールを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export verilog\n",
    "bb.load_networks(data_path, net)\n",
    "\n",
    "# 結果を出力\n",
    "\n",
    "with open(output_velilog_file, 'w') as f:\n",
    "    f.write('`timescale 1ns / 1ps\\n\\n')\n",
    "    bb.dump_verilog_lut_layers(f, module_name=rtl_module_name, net=net)\n",
    "\n",
    "# Simulation用ファイルに上書きコピー\n",
    "shutil.copyfile(output_velilog_file, sim_velilog_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション用データファイル作成\n",
    "with open(os.path.join(rtl_sim_path, 'mnist_test.txt'), 'w') as f:\n",
    "    bb.dump_verilog_readmemb_image_classification(f ,loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの内部の値を取得する\n",
    "\n",
    "Verilog以外の言語やFPGA以外に適用したい場合、接続とLUTテーブルの2つが取得できれば同じ計算をするモデルをインプリメントすることが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前準備\n",
    "そのままだと勾配はリセットされているので少しだけ逆伝搬を実施します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最新の保存データ読み込み\n",
    "bb.load_networks(data_path, net)\n",
    "\n",
    "# layer を取り出す\n",
    "layer0 = net[1]\n",
    "layer1 = net[2]\n",
    "layer2 = net[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接続を取得する\n",
    "\n",
    "LUTモデルは get_connection_list() にて接続行列を取得できます。<br>\n",
    "ここでの各出力ノードは、6つの入力と接続されており、layer0 の出力ノードは 1024 個あるので、1024x6 の行列が取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_mat = np.array(layer0.get_connection_list())\n",
    "print(connection_mat.shape)\n",
    "connection_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGA化する場合のLUTテーブルを取得する\n",
    "\n",
    "LUT化する場合のテーブルを取得します。<br>\n",
    "6入力のLUTモデルなので $ 2^6 = 64 $ 個のテーブルがあります。<br>\n",
    "モデル内に BatchNormalization 等を含む場合はそれらも加味して最終的にバイナリLUTにする場合に適した値を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_mat = np.array(layer0.get_lut_table_list())\n",
    "print(lut_mat.shape)\n",
    "lut_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重み行列を覗いてみる\n",
    "\n",
    "6入力のLUTモデルなので $ 2^6 = 64 $ 個のテーブルがあります。<br>\n",
    "W() にて bb.Tensor 型で取得可能で、numpy() にて ndarray に変換できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = layer0.W().numpy()\n",
    "print(W.shape)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配を覗いてみる\n",
    "\n",
    "同様に dW() でW の勾配が取得できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# そのままだとすべて0なので、1回だけbackward実施\n",
    "for images, labels in loader_test:\n",
    "    x_buf = bb.FrameBuffer.from_numpy(np.array(images).astype(np.float32))\n",
    "    t_buf = bb.FrameBuffer.from_numpy(np.identity(10)[np.array(labels)].astype(np.float32))\n",
    "    y_buf = net.forward(x_buf, train=True)\n",
    "    net.backward(loss.calculate(y_buf, t_buf))\n",
    "    break\n",
    "\n",
    "dW = layer0.dW().numpy()\n",
    "print(dW.shape)\n",
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGA用HLS(C言語高位合成)で使う為の出力\n",
    "\n",
    "内部データを取得する例としてHSL(C言語高位合成)用の出力を作ってみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lut_func_name(name, node):\n",
    "    return \"%s_lut_%d\"%(name, node)\n",
    "\n",
    "def dump_hls_lut_node4(f, name, lut, node):\n",
    "#    f.write(\"\\ninline ap_uint<1> %s(\\n\"%(make_lut_func_name(name, node)))\n",
    "    f.write(\"\\nap_uint<1> %s(\\n\"%(make_lut_func_name(name, node)))\n",
    "    n = lut.get_node_connection_size(node)\n",
    "    s = lut.get_lut_table_size(node)\n",
    "    \n",
    "    tbl = 0\n",
    "    for i in range(s):\n",
    "        if lut.get_lut_table(node ,i):\n",
    "            tbl += (1 << i)\n",
    "    \n",
    "    for i in range(n):\n",
    "        f.write(\"        ap_uint<1> in_data%d\"%(i))\n",
    "        if i < n-1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\")\\n\")\n",
    "    f.write(\"{\\n\")\n",
    "#   f.write(\"#pragma HLS inline\\n\")\n",
    "    f.write(\"    ap_uint<%d> index;\\n\"%(n))\n",
    "    for i in range(n):\n",
    "        f.write(\"    index[%d] = in_data%d;\\n\"%(i, i))\n",
    "    f.write(\"    return ((0x%016xLL >> index) & 1);\\n\"%tbl)\n",
    "    f.write(\"}\\n\\n\")\n",
    "\n",
    "def dump_hls_lut_node3(f, name, lut, node):\n",
    "    f.write(\"\\ninline ap_uint<1> %s(\\n\"%(make_lut_func_name(name, node)))\n",
    "#    f.write(\"\\nap_uint<1> %s(\\n\"%(make_lut_func_name(name, node)))\n",
    "    n = lut.get_node_connection_size(node)\n",
    "    s = lut.get_lut_table_size(node)\n",
    "    \n",
    "    tbl = 0\n",
    "    for i in range(s):\n",
    "        if lut.get_lut_table(node ,i):\n",
    "            tbl += (1 << i)\n",
    "    \n",
    "    for i in range(n):\n",
    "        f.write(\"        ap_uint<1> in_data%d\"%(i))\n",
    "        if i < n-1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\")\\n\")\n",
    "    f.write(\"{\\n\")\n",
    "#   f.write(\"#pragma HLS inline\\n\")\n",
    "    f.write(\"    ap_uint<%d> index;\\n\"%(n))\n",
    "    for i in range(n):\n",
    "        f.write(\"    index[%d] = in_data%d;\\n\"%(i, i))\n",
    "    f.write(\"    static Lut6Model table(0x%016xLL);\\n\"%(tbl))\n",
    "    f.write(\"    return table.Get(index);\\n\")\n",
    "    f.write(\"}\\n\\n\")\n",
    "\n",
    "def dump_hls_lut_node2(f, name, lut, node):\n",
    "    f.write(\"\\ninline ap_uint<1> %s(\\n\"%(make_lut_func_name(name, node)))\n",
    "    n = lut.get_node_connection_size(node)\n",
    "    s = lut.get_lut_table_size(node)\n",
    "    for i in range(n):\n",
    "        f.write(\"        ap_uint<1> in_data%d\"%(i))\n",
    "        if i < n-1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\")\\n\")\n",
    "    f.write(\"{\\n\")\n",
    "    f.write(\"#pragma HLS inline\\n\\n\")\n",
    "    f.write(\"    ap_uint<%d> index;\\n\"%(n))\n",
    "    for i in range(n):\n",
    "        f.write(\"    index[%d] = in_data%d;\\n\"%(i, i))\n",
    "    f.write(\"    \\n\")\n",
    "    f.write(\"    const ap_uint<1> table[%d] = {\"%(s))\n",
    "    for i in range(s):\n",
    "        f.write(\"%d,\"%(lut.get_lut_table(node ,i)))\n",
    "    f.write(\"};\\n\")\n",
    "#    for i in range(s):\n",
    "#        f.write(\"    table[%d] = %d;\\n\"%(i, lut.get_lut_table(node ,i)))\n",
    "#    f.write(\"    \\n\")\n",
    "#   f.write(\"    #pragma HLS resource variable=table core=ROM_1P_LUTRAM\\n\")\n",
    "    f.write(\"    #pragma HLS bind_storage variable=table type=ROM_1P impl=LUTRAM\\n\")\n",
    "    f.write(\"    return table[index];\\n\")\n",
    "    f.write(\"}\\n\\n\")\n",
    "\n",
    "def dump_hls_lut_node(f, name, lut, node):\n",
    "    f.write(\"\\ninline ap_uint<1> %s(\\n\"%(make_lut_func_name(name, node)))\n",
    "    n = lut.get_node_connection_size(node)\n",
    "    s = lut.get_lut_table_size(node)\n",
    "    \n",
    "    tbl = 0\n",
    "    for i in range(s):\n",
    "        if lut.get_lut_table(node ,i):\n",
    "            tbl += (1 << i)\n",
    "    \n",
    "    for i in range(n):\n",
    "        f.write(\"        ap_uint<1> in_data%d\"%(i))\n",
    "        if i < n-1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\")\\n\")\n",
    "    f.write(\"{\\n\")\n",
    "    f.write(\"#pragma HLS inline\\n\")\n",
    "    f.write(\"    ap_uint<%d> index;\\n\"%(n))\n",
    "    for i in range(n):\n",
    "        f.write(\"    index[%d] = in_data%d;\\n\"%(i, i))\n",
    "    f.write(\"    const ap_uint<%d> table= 0x%016xLL;\\n\"%(s, tbl))\n",
    "    f.write(\"    return table[index];\\n\")\n",
    "    f.write(\"}\\n\\n\")\n",
    "\n",
    "def dump_hls_lut(f, name, lut):\n",
    "    ins  = lut.get_input_node_size()\n",
    "    outs = lut.get_output_node_size()\n",
    "    for node in range(outs):\n",
    "        dump_hls_lut_node3(f, name, lut, node)\n",
    "    \n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"inline ap_uint<%d> %s(ap_uint<%d> in_data)\\n\"%(outs, name, ins))\n",
    "    f.write(\"{\\n\")\n",
    "    f.write(\"    ap_uint<%d>  out_data;\\n\"%(outs))\n",
    "    for node in range(outs):\n",
    "        f.write(\"    out_data[%d] = %s(\"%(node, make_lut_func_name(name, node)))\n",
    "        n = lut.get_node_connection_size(node)\n",
    "        for i in range(n):\n",
    "            f.write(\"in_data[%d]\"%(lut.get_node_connection_index(node, i)))\n",
    "            if i < n-1: \n",
    "                f.write(\",\")\n",
    "            else:\n",
    "                f.write(\");\\n\")\n",
    "    f.write(\"    return out_data;\\n\")   \n",
    "    f.write(\"}\\n\\n\")\n",
    "\n",
    "# 学習済みを読みなおす    \n",
    "bb.load_networks(data_path, net)\n",
    "with open(\"MnistDifferentiableLutSimpleHls.h\", \"w\") as f:\n",
    "    f.write('#include \"ap_int.h\"\\n\\n')\n",
    "    for i in range(1, 3):\n",
    "        dump_hls_lut(f, \"mnist_layer%d\"%i, net[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp MnistDifferentiableLutSimpleHls.h ../../hls/mnist/mnist_simple/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "with open('mnist_hls_test.txt', 'w') as f:\n",
    "    for images, labels in loader_test:\n",
    "        x_buf = np.array(images).astype(np.float32)\n",
    "        t_buf = np.array(labels)\n",
    "        for i in range(x_buf.shape[0]):\n",
    "            f.write(\"%d\"%t_buf[i])\n",
    "            for y in range(x_buf.shape[2]):\n",
    "                for x in range(x_buf.shape[3]):\n",
    "                    f.write(\" %d\"%(x_buf[i, 0, y, x] > 0.5))\n",
    "            f.write(\"\\n\")\n",
    "            num += 1\n",
    "        if num > 1024:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! copy MnistDifferentiableLutSimpleHls.h \\\\wsl$\\Ubuntu-20.04\\home\\ryuji\\git-work\\jelly_develop\\projects\\ultra96v2_hls_test\\hls_\\src\n",
    "! copy mnist_hls_test.txt \\\\wsl$\\Ubuntu-20.04\\home\\ryuji\\git-work\\jelly_develop\\projects\\ultra96v2_hls_test\\hls_\\testbench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! copy MnistDifferentiableLutSimpleHls.h ..\\..\\hls\\mnist\\mnist_simple\\src\n",
    "! copy mnist_hls_test.txt                ..\\..\\hls\\mnist\\mnist_simple\\testbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42ac7cce52f856a953029bf8d6268c151b6ce75ee0dd17c7a9f0cf75b5e0010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
